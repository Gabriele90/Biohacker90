{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Working With Splitters.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPY45XDkDxkB7txJNoTtlfU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gabriele90/Biohacker90/blob/main/Working_With_Splitters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9uVsVaION2c"
      },
      "source": [
        "**Working With Splitters**\n",
        "\n",
        "When using machine learning, you typically divide your data into training, validation, and test sets. The MoleculeNet loaders do this automatically. But how should you divide up the data?\n",
        "There are many ways of splitting up data, and which one you choose can have a big impact on the reliability of your results. This tutorial introduces some of the splitting methods provided by DeepChem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ArW3tlsOYDN"
      },
      "source": [
        "**Setup**\n",
        "To run DeepChem within Colab, you'll need to run the following installation commands. This will take about 5 minutes to run to completion and install your environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVZ6Gk2_OKJL",
        "outputId": "a3b074be-7b07-4537-95d1-9dbe8c3b750b"
      },
      "source": [
        "!curl -Lo conda_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab_install.py\n",
        "import conda_installer\n",
        "conda_installer.install()\n",
        "!/root/miniconda/bin/conda info -e"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3490  100  3490    0     0  19497      0 --:--:-- --:--:-- --:--:-- 19497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "add /root/miniconda/lib/python3.6/site-packages to PYTHONPATH\n",
            "python version: 3.6.9\n",
            "fetching installer from https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "done\n",
            "installing miniconda to /root/miniconda\n",
            "done\n",
            "installing rdkit, openmm, pdbfixer\n",
            "added omnia to channels\n",
            "added conda-forge to channels\n",
            "done\n",
            "conda packages installation finished!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "# conda environments:\n",
            "#\n",
            "base                  *  /root/miniconda\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxN3e-EpOeTb"
      },
      "source": [
        "!pip install --pre deepchem\n",
        "import deepchem\n",
        "deepchem.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtWugvOzOgXi"
      },
      "source": [
        "**Splitters**\n",
        "\n",
        "In DeepChem, a method of splitting samples into multiple datasets is defined by a Splitter object. Choosing an appropriate method for your data is very important. Otherwise, your trained model may seem to work much better than it really does.\n",
        "\n",
        "Consider a typical drug development pipeline. You might begin by screening many thousands of molecules to see if they bind to your target of interest. Once you find one that seems to work, you try to optimize it by testing thousands of minor variations on it, looking for one that binds more strongly. Then perhaps you test it in animals and find it has unacceptable toxicity, so you try more variations to fix the problems.\n",
        "\n",
        "This has an important consequence for chemical datasets: they often include lots of molecules that are very similar to each other. If you split the data into training and test sets in a naive way, the training set will include many molecules that are very similar to the ones in the test set, even if they are not exactly identical. As a result, the model may do very well on the test set, but then fail badly when you try to use it on other data that is less similar to the training data.\n",
        "\n",
        "Let's take a look at a few of the splitters found in DeepChem.\n",
        "\n",
        "**RandomSplitter**\n",
        "\n",
        "This is one of the simplest splitters. It just selects samples for the training, validation, and test sets in a completely random way.\n",
        "\n",
        "Didn't we just say that's a bad idea? Well, it depends on your data. If every sample is truly independent of every other, then this is just as good a way as any to split the data. There is no universally best choice of splitter. It all depends on your particular dataset, and for some datasets this is a fine choice.\n",
        "\n",
        "**RandomStratifiedSplitter**\n",
        "Some datasets are very unbalanced: only a tiny fraction of all samples are positive. In that case, random splitting may sometimes lead to the validation or test set having few or even no positive samples for some tasks. That makes it unable to evaluate performance.\n",
        "\n",
        "RandomStratifiedSplitter addresses this by dividing up the positive and negative samples evenly. If you ask for a 80/10/10 split, the validation and test sets will contain not just 10% of samples, but also 10% of the positive samples for each task.\n",
        "\n",
        "**ScaffoldSplitter**\n",
        "\n",
        "This splitter tries to address the problem discussed above where many molecules are very similar to each other. It identifies the scaffold that forms the core of each molecule, and ensures that all molecules with the same scaffold are put into the same dataset. This is still not a perfect solution, since two molecules may have different scaffolds but be very similar in other ways, but it usually is a large improvement over random splitting.\n",
        "\n",
        "**ButinaSplitter**\n",
        "\n",
        "This is another splitter that tries to address the problem of similar molecules. It clusters them based on their molecular fingerprints, so that ones with similar fingerprints will tend to be in the same dataset. The time required by this splitting algorithm scales as the square of the number of molecules, so it is mainly useful for small to medium sized datasets.\n",
        "\n",
        "**SpecifiedSplitter**\n",
        "\n",
        "This splitter leaves everything up to the user. You tell it exactly which samples to put in each dataset. This is useful when you know in advance that a particular splitting is appropriate for your data.\n",
        "\n",
        "An example is temporal splitting. Consider a research project where you are continually generating and testing new molecules. As you gain more data, you periodically retrain your model on the steadily growing dataset, then use it to predict results for other not yet tested molecules. A good way of validating whether this works is to pick a particular cutoff date, train the model on all data you had at that time, and see how well it predicts other data that was generated later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGtDLT6vOxIs"
      },
      "source": [
        "**Effect of Using Different Splitters**\n",
        "\n",
        "Let's look at an example. We will load the Tox21 toxicity dataset using random, scaffold, and Butina splitting. For each one we train a model and evaluate it on the training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL8smpaPUkea",
        "outputId": "abf47373-4328-447d-f171-2b36ba17dec4"
      },
      "source": [
        "!pip install --pre deepchem"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepchem\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/5c/ab3d2efc10f29b71e2b09d0ddc62a767c5f2ab0007914e673b8ff2c2a905/deepchem-2.4.0rc1.dev20201116174536.tar.gz (401kB)\n",
            "\r\u001b[K     |▉                               | 10kB 14.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20kB 20.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30kB 10.9MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40kB 9.2MB/s eta 0:00:01\r\u001b[K     |████                            | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 92kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 102kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 112kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 122kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 133kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 143kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 153kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 163kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 174kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 184kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 194kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 204kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 215kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 225kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 235kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 245kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 256kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 266kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 276kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 286kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 296kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 307kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 317kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 327kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 337kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 348kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 358kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 368kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 378kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 389kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 399kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 409kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from deepchem) (0.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from deepchem) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->deepchem) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->deepchem) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\n",
            "Building wheels for collected packages: deepchem\n",
            "  Building wheel for deepchem (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepchem: filename=deepchem-2.4.0rc1.dev20201118122650-cp36-none-any.whl size=516159 sha256=f0f6f78d43a7237852de5a6318c3611b517c492835539e9e8f1b2d49ea579dd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/62/46/62d0ff22abaf0778aa1ef94cde4282cb45fcb190f5cdd4ec75\n",
            "Successfully built deepchem\n",
            "Installing collected packages: deepchem\n",
            "Successfully installed deepchem-2.4.0rc1.dev20201118122650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRMmFkzsUO8E",
        "outputId": "11925ce3-a9b0-44b8-961a-00f3d3f0d4ab"
      },
      "source": [
        "import deepchem as dc\n",
        "\n",
        "splitters = ['random', 'scaffold', 'butina']\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "for splitter in splitters:\n",
        "    tasks, datasets, transformers = dc.molnet.load_tox21(featurizer='ECFP', split=splitter)\n",
        "    train_dataset, valid_dataset, test_dataset = datasets\n",
        "    model = dc.models.MultitaskClassifier(n_tasks=len(tasks), n_features=1024, layer_sizes=[1000])\n",
        "    model.fit(train_dataset, nb_epoch=10)\n",
        "    print('splitter:', splitter)\n",
        "    print('training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
        "    print('test set score:', model.evaluate(test_dataset, [metric], transformers))\n",
        "    print()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'split' is deprecated.  Use 'splitter' instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "splitter: random\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "'split' is deprecated.  Use 'splitter' instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training set score: {'roc_auc_score': 0.9548520301680785}\n",
            "test set score: {'roc_auc_score': 0.7976091587543174}\n",
            "\n",
            "splitter: scaffold\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "'split' is deprecated.  Use 'splitter' instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training set score: {'roc_auc_score': 0.959000585603718}\n",
            "test set score: {'roc_auc_score': 0.6833028973520859}\n",
            "\n",
            "splitter: butina\n",
            "training set score: {'roc_auc_score': 0.9579764162556358}\n",
            "test set score: {'roc_auc_score': 0.6054570969646899}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_izay28VRBS"
      },
      "source": [
        "All of them produce very similar performance on the training set, but the random splitter has much higher performance on the test set. Scaffold splitting has a lower test set score, and Butina splitting is even lower. Does that mean random splitting is better? No! It means random splitting doesn't give you an accurate measure of how well your model works. Because the test set contains lots of molecules that are very similar to ones in the training set, it isn't truly independent. It makes the model appear to work better than it really does. Scaffold splitting and Butina splitting give a better indication of what you can expect on independent data in the future."
      ]
    }
  ]
}