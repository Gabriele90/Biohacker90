{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+tnl0fr5ZX/FmRiBf0+pR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gabriele90/Biohacker90/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ8LREPc5I2K"
      },
      "source": [
        "**Introduction to Graph Convolutions**\n",
        "\n",
        "Graph convolutions are one of the most powerful deep learning tools for working with molecular data. The reason for this is that molecules can be naturally viewed as graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4VUJiZ15WrA"
      },
      "source": [
        "**Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc2kDaMO5FAE",
        "outputId": "5d3fabdf-b8d9-4ad0-d48d-a1c5c61135e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!curl -Lo conda_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab_install.py\n",
        "import conda_installer\n",
        "conda_installer.install()\n",
        "!/root/miniconda/bin/conda info -e"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3490  100  3490    0     0   6803      0 --:--:-- --:--:-- --:--:--  6789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "add /root/miniconda/lib/python3.6/site-packages to PYTHONPATH\n",
            "python version: 3.6.9\n",
            "fetching installer from https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "done\n",
            "installing miniconda to /root/miniconda\n",
            "done\n",
            "installing rdkit, openmm, pdbfixer\n",
            "added omnia to channels\n",
            "added conda-forge to channels\n",
            "done\n",
            "conda packages installation finished!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "# conda environments:\n",
            "#\n",
            "base                  *  /root/miniconda\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGBRng_55cK5",
        "outputId": "2e6cf649-a900-435e-c923-12b95840525d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install --pre deepchem"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepchem\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/71/ee73bf319e813620c6cc172c93edca6b605e4332ee0f48905a55cdf6bfb0/deepchem-2.4.0rc1.dev20201105213837.tar.gz (401kB)\n",
            "\r\u001b[K     |▉                               | 10kB 7.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████                            | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 92kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 307kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 317kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 327kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 337kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 348kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 358kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 368kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 378kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 389kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 399kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 409kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from deepchem) (0.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from deepchem) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->deepchem) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->deepchem) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\n",
            "Building wheels for collected packages: deepchem\n",
            "  Building wheel for deepchem (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepchem: filename=deepchem-2.4.0rc1.dev20201106093625-cp36-none-any.whl size=509255 sha256=3b934bae16e13723f43634a893b364ba76af2daa8343b495c03be0c4516bb6a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/61/f0/a78560a55905ec6898220b33f3bdacf9d186339fd1d7e5a264\n",
            "Successfully built deepchem\n",
            "Installing collected packages: deepchem\n",
            "Successfully installed deepchem-2.4.0rc1.dev20201106093625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yET7kNtj5nyr"
      },
      "source": [
        "**Training a GraphConvModel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_NaTCKX5v8I"
      },
      "source": [
        "Let's use the MoleculeNet suite to load the Tox21 dataset. To featurize the data in a way that graph convolutional networks can use, we set the featurizer option to 'GraphConv'. The MoleculeNet call returns a training set, a validation set, and a test set for us to use. It also returns tasks, a list of the task names, and transformers, a list of data transformations that were applied to preprocess the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxwpQTMq5xSh"
      },
      "source": [
        "import deepchem as dc\n",
        "\n",
        "tasks, datasets, transformers = dc.molnet.load_tox21(featurizer='GraphConv')\n",
        "train_dataset, valid_dataset, test_dataset = datasets"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XZXuLgd7H6S"
      },
      "source": [
        "Let's now train a graph convolutional network on this dataset. DeepChem has the class GraphConvModel that wraps a standard graph convolutional architecture underneath the hood for user convenience. Let's instantiate an object of this class and train it on our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA8cS-pr7I6u",
        "outputId": "f566f74d-821e-4f4c-90ed-dda574383e7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n_tasks = len(tasks)\n",
        "model = dc.models.GraphConvModel(n_tasks, mode='classification')\n",
        "model.fit(train_dataset, nb_epoch=50)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.274965877532959"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzWNRE0p7SRa"
      },
      "source": [
        "Let's try to evaluate the performance of the model we've trained. For this, we need to define a metric, a measure of model performance. dc.metrics holds a collection of metrics already. For this dataset, it is standard to use the ROC-AUC score, the area under the receiver operating characteristic curve (which measures the tradeoff between precision and recall)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKA5pzii7Rj3",
        "outputId": "325020e1-4755-4bf8-ec3b-30c543949119",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "print('Training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
        "print('Test set score:', model.evaluate(test_dataset, [metric], transformers))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: {'roc_auc_score': 0.9715942789824119}\n",
            "Test set score: {'roc_auc_score': 0.7031399045204187}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3aEopuj7duC"
      },
      "source": [
        "The results are pretty good, and GraphConvModel is very easy to use. But what's going on under the hood? Could we build GraphConvModel ourselves? Of course! DeepChem provides Keras layers for all the calculations involved in a graph convolution. We are going to apply the following layers from DeepChem.\n",
        "\n",
        "GraphConv layer: This layer implements the graph convolution. The graph convolution combines per-node feature vectures in a nonlinear fashion with the feature vectors for neighboring nodes. This \"blends\" information in local neighborhoods of a graph.\n",
        "\n",
        "GraphPool layer: This layer does a max-pooling over the feature vectors of atoms in a neighborhood. You can think of this layer as analogous to a max-pooling layer for 2D convolutions but which operates on graphs instead.\n",
        "\n",
        "GraphGather: Many graph convolutional networks manipulate feature vectors per graph-node. For a molecule for example, each node might represent an atom, and the network would manipulate atomic feature vectors that summarize the local chemistry of the atom. However, at the end of the application, we will likely want to work with a molecule level feature representation. This layer creates a graph level feature vector by combining all the node-level feature vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKG3hTDD7eoL"
      },
      "source": [
        "from deepchem.models.layers import GraphConv, GraphPool, GraphGather\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "class MyGraphConvModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MyGraphConvModel, self).__init__()\n",
        "    self.gc1 = GraphConv(128, activation_fn=tf.nn.tanh)\n",
        "    self.batch_norm1 = layers.BatchNormalization()\n",
        "    self.gp1 = GraphPool()\n",
        "\n",
        "    self.gc2 = GraphConv(128, activation_fn=tf.nn.tanh)\n",
        "    self.batch_norm2 = layers.BatchNormalization()\n",
        "    self.gp2 = GraphPool()\n",
        "\n",
        "    self.dense1 = layers.Dense(256, activation=tf.nn.tanh)\n",
        "    self.batch_norm3 = layers.BatchNormalization()\n",
        "    self.readout = GraphGather(batch_size=batch_size, activation_fn=tf.nn.tanh)\n",
        "\n",
        "    self.dense2 = layers.Dense(n_tasks*2)\n",
        "    self.logits = layers.Reshape((n_tasks, 2))\n",
        "    self.softmax = layers.Softmax()\n",
        "  def call(self, inputs):\n",
        "    gc1_output = self.gc1(inputs)\n",
        "    batch_norm1_output = self.batch_norm1(gc1_output)\n",
        "    gp1_output = self.gp1([batch_norm1_output] + inputs[1:])\n",
        "\n",
        "    gc2_output = self.gc2([gp1_output] + inputs[1:])\n",
        "    batch_norm2_output = self.batch_norm1(gc2_output)\n",
        "    gp2_output = self.gp2([batch_norm2_output] + inputs[1:])\n",
        "\n",
        "    dense1_output = self.dense1(gp2_output)\n",
        "    batch_norm3_output = self.batch_norm3(dense1_output)\n",
        "    readout_output = self.readout([batch_norm3_output] + inputs[1:])\n",
        "\n",
        "    logits_output = self.logits(self.dense2(readout_output))\n",
        "    return self.softmax(logits_output)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byZ6oXC87pVx"
      },
      "source": [
        "There are two convolutional blocks, each consisting of a GraphConv, followed by batch normalization, followed by a GraphPool to do max pooling. We finish up with a dense layer, another batch normalization, a GraphGather to combine the data from all the different nodes, and a final dense layer to produce the global output.\n",
        "\n",
        "Let's now create the DeepChem model which will be a wrapper around the Keras model that we just created. We will also specify the loss function so the model know the objective to minimize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riavu3Mw7p34"
      },
      "source": [
        "model = dc.models.KerasModel(MyGraphConvModel(), loss=dc.models.losses.CategoricalCrossEntropy())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Iw5XELd7xAw"
      },
      "source": [
        "What are the inputs to this model? A graph convolution requires a complete description of each molecule, including the list of nodes (atoms) and a description of which ones are bonded to each other. In fact, if we inspect the dataset we see that the feature array contains Python objects of type ConvMol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jMuy9FQ7xqD",
        "outputId": "d91db75b-ddfe-4d95-9873-e6123778f9a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_dataset.X[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<deepchem.feat.mol_graphs.ConvMol at 0x7f797a6decf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ks-NpNb8B2N"
      },
      "source": [
        "\n",
        "Models expect arrays of numbers as their inputs, not Python objects. We must convert the ConvMol objects into the particular set of arrays expected by the GraphConv, GraphPool, and GraphGather layers. Fortunately, the ConvMol class includes the code to do this, as well as to combine all the molecules in a batch to create a single set of arrays.\n",
        "\n",
        "The following code creates a Python generator that given a batch of data generates the lists of inputs, labels, and weights whose values are Numpy arrays. atom_features holds a feature vector of length 75 for each atom. The other inputs are required to support minibatching in TensorFlow. degree_slice is an indexing convenience that makes it easy to locate atoms from all molecules with a given degree. membership determines the membership of atoms in molecules (atom i belongs to molecule membership[i])."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHY4OIKA8CbS"
      },
      "source": [
        "from deepchem.metrics import to_one_hot\n",
        "from deepchem.feat.mol_graphs import ConvMol\n",
        "import numpy as np\n",
        "\n",
        "def data_generator(dataset, epochs=1):\n",
        "  for ind, (X_b, y_b, w_b, ids_b) in enumerate(dataset.iterbatches(batch_size, epochs,\n",
        "                                                                   deterministic=False, pad_batches=True)):\n",
        "    multiConvMol = ConvMol.agglomerate_mols(X_b)\n",
        "    inputs = [multiConvMol.get_atom_features(), multiConvMol.deg_slice, np.array(multiConvMol.membership)]\n",
        "    for i in range(1, len(multiConvMol.get_deg_adjacency_lists())):\n",
        "      inputs.append(multiConvMol.get_deg_adjacency_lists()[i])\n",
        "    labels = [to_one_hot(y_b.flatten(), 2).reshape(-1, n_tasks, 2)]\n",
        "    weights = [w_b]\n",
        "    yield (inputs, labels, weights)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDgBrVD18H2w"
      },
      "source": [
        "Now, we can train the model using fit_generator(generator) which will use the generator we've defined to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vq0G2tt8IWK",
        "outputId": "d02636f8-abf6-49b8-f0b9-73e6b8bbcb72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit_generator(data_generator(train_dataset, epochs=50))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 3132 calls to <function KerasModel._create_gradient_fn.<locals>.apply_gradient_for_batch at 0x7f79765247b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22591007232666016"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iaYmxIL8NRS"
      },
      "source": [
        "Now that we have trained our graph convolutional method, let's evaluate its performance. We again have to use our defined generator to evaluate model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl7rpZJ78N11",
        "outputId": "98fc0349-0e44-43b5-8983-e9219c0cbacd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Training set score:', model.evaluate_generator(data_generator(train_dataset), [metric], transformers))\n",
        "print('Test set score:', model.evaluate_generator(data_generator(test_dataset), [metric], transformers))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: {'roc_auc_score': 0.8017301451802408}\n",
            "Test set score: {'roc_auc_score': 0.6285294095272476}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iffXH1tb8V3P"
      },
      "source": [
        "\n",
        "Success! The model we've constructed behaves nearly identically to GraphConvModel."
      ]
    }
  ]
}